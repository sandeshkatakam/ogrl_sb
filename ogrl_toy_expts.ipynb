{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bda3fb2",
   "metadata": {},
   "source": [
    "# Offline Goal-Conditioned RL Experiment Setup\n",
    "\n",
    "This notebook demonstrates a complete setup for offline goal-conditioned RL experiments using a simple 2D navigation environment.\n",
    "\n",
    "## Components:\n",
    "1. **Toy Environment**: Simple 2D navigation with goal conditioning\n",
    "2. **Trajectory Generator**: Collect offline trajectories with different policies\n",
    "3. **Data Loader**: Composable PyTorch data loader for offline RL\n",
    "4. **Goal Conditioning**: Support for goal relabeling and different sampling strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4517fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Import our custom modules\n",
    "from toy_env import make_toy_env, Toy2DNavigationEnv, GoalConditionedWrapper\n",
    "from trajectory_generator import TrajectoryGenerator, create_expert_dataset, ExpertPolicy\n",
    "from data_loader import OfflineRLDataset, GoalConditionedDataset, OfflineRLDataLoader, create_data_loaders\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61a432",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's create and test our toy 2D navigation environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af304d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created!\n",
      "Observation space: Box(0.0, 10.0, (4,), float32)\n",
      "Action space: Discrete(5)\n",
      "\n",
      "Initial observation: [0.781773  7.1862245 2.93081   2.539869 ]\n",
      "Goal: [2.93081  2.539869]\n",
      "Distance to goal: 5.12\n",
      "Step 0: action=1, obs=[0.93446875 6.4198904  2.93081    2.539869  ], reward=0.0, done=False\n",
      "Step 1: action=4, obs=[0.9492548 6.4899616 2.93081   2.539869 ], reward=0.0, done=False\n",
      "Step 2: action=1, obs=[0.89271706 5.504059   2.93081    2.539869  ], reward=0.0, done=False\n",
      "Step 3: action=0, obs=[0.91625524 6.35176    2.93081    2.539869  ], reward=0.0, done=False\n",
      "Step 4: action=2, obs=[0.       6.357038 2.93081  2.539869], reward=0.0, done=False\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = make_toy_env(\n",
    "    grid_size=10,\n",
    "    max_steps=50,\n",
    "    goal_conditioned=True,\n",
    "    action_noise=0.1\n",
    ")\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Test the environment\n",
    "obs, info = env.reset()\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "print(f\"Goal: {info['goal']}\")\n",
    "print(f\"Distance to goal: {info['distance_to_goal']:.2f}\")\n",
    "\n",
    "# Take a few random steps\n",
    "for step in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {step}: action={action}, obs={obs}, reward={reward}, done={terminated or truncated}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode ended. Success: {info['success']}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b11af",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Now let's generate offline trajectories using different policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e967ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating expert dataset...\n",
      "Generating 500 trajectories...\n",
      "Generated 0/500 trajectories\n",
      "Generated 100/500 trajectories\n",
      "Generated 200/500 trajectories\n",
      "Generated 300/500 trajectories\n",
      "Generated 400/500 trajectories\n",
      "Generated 500 trajectories\n",
      "Dataset saved to expert_dataset.pkl\n",
      "Expert dataset created:\n",
      "  Success rate: 50.80%\n",
      "  Average length: 25.9\n",
      "  Total trajectories: 500\n",
      "\n",
      "Generating random dataset...\n",
      "Generating 500 trajectories...\n",
      "Generated 0/500 trajectories\n",
      "Generated 100/500 trajectories\n",
      "Generated 200/500 trajectories\n",
      "Generated 300/500 trajectories\n",
      "Generated 400/500 trajectories\n",
      "Generated 500 trajectories\n",
      "Dataset saved to random_dataset.pkl\n",
      "Expert Dataset:\n",
      "  Success rate: 50.80%\n",
      "  Average length: 25.9\n",
      "  Total trajectories: 500\n",
      "Random Dataset:\n",
      "  Success rate: 35.20%\n",
      "  Average length: 35.1\n",
      "  Total trajectories: 500\n"
     ]
    }
   ],
   "source": [
    "# Generate expert dataset\n",
    "print(\"Generating expert dataset...\")\n",
    "expert_trajectories = create_expert_dataset(\n",
    "    env=env,\n",
    "    num_trajectories=500,\n",
    "    save_path=\"expert_dataset.pkl\",\n",
    "    noise=0.2  # Add some noise to make it more realistic\n",
    ")\n",
    "\n",
    "# Generate random dataset for comparison\n",
    "print(\"\\nGenerating random dataset...\")\n",
    "random_generator = TrajectoryGenerator(env)\n",
    "random_trajectories = random_generator.generate_dataset(\n",
    "    num_trajectories=500,\n",
    "    max_steps=50,\n",
    "    save_path=\"random_dataset.pkl\"\n",
    ")\n",
    "\n",
    "# Print statistics\n",
    "def print_dataset_stats(trajectories, name):\n",
    "    success_rate = sum(traj.success for traj in trajectories) / len(trajectories)\n",
    "    avg_length = np.mean([traj.length for traj in trajectories])\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Success rate: {success_rate:.2%}\")\n",
    "    print(f\"  Average length: {avg_length:.1f}\")\n",
    "    print(f\"  Total trajectories: {len(trajectories)}\")\n",
    "\n",
    "print_dataset_stats(expert_trajectories, \"Expert Dataset\")\n",
    "print_dataset_stats(random_trajectories, \"Random Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f748e01",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "Create PyTorch data loaders for training offline RL algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b980e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders...\n",
      "Train loader: 164 batches\n",
      "Val loader: 39 batches\n",
      "\n",
      "Batch shapes:\n",
      "  Observations: torch.Size([64, 4])\n",
      "  Actions: torch.Size([64])\n",
      "  Next observations: torch.Size([64, 4])\n",
      "  Rewards: torch.Size([64])\n",
      "  Terminals: torch.Size([64])\n",
      "  Goals: torch.Size([64, 2])\n",
      "\n",
      "Example transitions:\n",
      "  Transition 0:\n",
      "    State: tensor([0.1382, 0.0000])\n",
      "    Goal: tensor([ 0.0952, -0.8507])\n",
      "    Action: 1\n",
      "    Next state: tensor([0.3081, 0.0000])\n",
      "    Reward: 0.0\n",
      "    Terminal: False\n",
      "\n",
      "  Transition 1:\n",
      "    State: tensor([0.1063, 0.0000])\n",
      "    Goal: tensor([-0.3411, -0.4085])\n",
      "    Action: 1\n",
      "    Next state: tensor([0., 0.])\n",
      "    Reward: 0.0\n",
      "    Terminal: False\n",
      "\n",
      "  Transition 2:\n",
      "    State: tensor([0.0071, 0.0000])\n",
      "    Goal: tensor([ 0.0198, -0.6909])\n",
      "    Action: 1\n",
      "    Next state: tensor([0.0911, 0.0000])\n",
      "    Reward: 0.0\n",
      "    Terminal: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeshkatakam/Documents/local_github_repos/ogrl_sb/ogrl_sb_arm64/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders for expert dataset\n",
    "print(\"Creating data loaders...\")\n",
    "\n",
    "# Standard goal-conditioned dataset\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    trajectories=expert_trajectories,\n",
    "    train_ratio=0.8,\n",
    "    batch_size=64,\n",
    "    goal_conditioned=True,\n",
    "    goal_relabeling=True,\n",
    "    goal_sampling_strategy='future'  # HER-style goal relabeling\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Observations: {batch.observations.shape}\")\n",
    "print(f\"  Actions: {batch.actions.shape}\")\n",
    "print(f\"  Next observations: {batch.next_observations.shape}\")\n",
    "print(f\"  Rewards: {batch.rewards.shape}\")\n",
    "print(f\"  Terminals: {batch.terminals.shape}\")\n",
    "print(f\"  Goals: {batch.goals.shape}\")\n",
    "\n",
    "# Show some example transitions\n",
    "print(f\"\\nExample transitions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Transition {i}:\")\n",
    "    print(f\"    State: {batch.observations[i][:2]}\")\n",
    "    print(f\"    Goal: {batch.goals[i]}\")\n",
    "    print(f\"    Action: {batch.actions[i]}\")\n",
    "    print(f\"    Next state: {batch.next_observations[i][:2]}\")\n",
    "    print(f\"    Reward: {batch.rewards[i]}\")\n",
    "    print(f\"    Terminal: {batch.terminals[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a64b6",
   "metadata": {},
   "source": [
    "## 4. Different Goal Sampling Strategies\n",
    "\n",
    "Compare different goal sampling strategies for goal-conditioned RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8da77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UNIFORM Goal Sampling ===\n",
      "  Goal range: [-1.00, 0.98]\n",
      "  Goal std: 0.55\n",
      "  Unique goals: 96\n",
      "  Sample goals: [[-0.56691396  0.8602849 ]\n",
      " [-0.47928172 -0.16742727]\n",
      " [-0.3488859  -0.36081022]\n",
      " [ 0.7733705  -0.86309946]\n",
      " [ 0.97817135 -0.5976935 ]]\n",
      "\n",
      "=== FUTURE Goal Sampling ===\n",
      "  Goal range: [-1.00, 0.98]\n",
      "  Goal std: 0.54\n",
      "  Unique goals: 100\n",
      "  Sample goals: [[ 0.77988     0.3095437 ]\n",
      " [ 0.77988     0.3095437 ]\n",
      " [ 0.77988     0.3095437 ]\n",
      " [ 0.53413177  0.70619184]\n",
      " [-0.26943734 -0.09547954]]\n",
      "\n",
      "=== FINAL Goal Sampling ===\n",
      "  Goal range: [-1.00, 0.98]\n",
      "  Goal std: 0.54\n",
      "  Unique goals: 100\n",
      "  Sample goals: [[ 0.77988     0.3095437 ]\n",
      " [ 0.77988     0.3095437 ]\n",
      " [ 0.77988     0.3095437 ]\n",
      " [ 0.53413177  0.70619184]\n",
      " [-0.26943734 -0.09547954]]\n"
     ]
    }
   ],
   "source": [
    "# Compare different goal sampling strategies\n",
    "strategies = ['uniform', 'future', 'final']\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n=== {strategy.upper()} Goal Sampling ===\")\n",
    "    \n",
    "    # Create dataset with specific strategy\n",
    "    dataset = GoalConditionedDataset(\n",
    "        trajectories=expert_trajectories[:100],  # Use subset for faster processing\n",
    "        goal_relabeling=True,\n",
    "        goal_sampling_strategy=strategy\n",
    "    )\n",
    "    \n",
    "    # Create data loader\n",
    "    loader = OfflineRLDataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Analyze goal distribution\n",
    "    all_goals = []\n",
    "    for batch in loader:\n",
    "        all_goals.extend(batch.goals.numpy())\n",
    "    \n",
    "    all_goals = np.array(all_goals)\n",
    "    \n",
    "    print(f\"  Goal range: [{all_goals.min():.2f}, {all_goals.max():.2f}]\")\n",
    "    print(f\"  Goal std: {all_goals.std():.2f}\")\n",
    "    print(f\"  Unique goals: {len(np.unique(all_goals, axis=0))}\")\n",
    "    \n",
    "    # Show some example goals\n",
    "    print(f\"  Sample goals: {all_goals[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0f28b",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Visualize the environment and some example trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d59a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some example trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot expert trajectories\n",
    "for i in range(3):\n",
    "    traj = expert_trajectories[i]\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract positions\n",
    "    positions = np.array(traj.observations)[:, :2]\n",
    "    goal = traj.goals[0]\n",
    "    \n",
    "    # Plot trajectory\n",
    "    ax.plot(positions[:, 0], positions[:, 1], 'b-', alpha=0.7, linewidth=2)\n",
    "    ax.scatter(positions[0, 0], positions[0, 1], c='green', s=100, marker='o', label='Start', zorder=5)\n",
    "    ax.scatter(positions[-1, 0], positions[-1, 1], c='red', s=100, marker='s', label='End', zorder=5)\n",
    "    ax.scatter(goal[0], goal[1], c='orange', s=100, marker='*', label='Goal', zorder=5)\n",
    "    \n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_title(f'Expert Trajectory {i+1} (Success: {traj.success})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot random trajectories\n",
    "for i in range(3):\n",
    "    traj = random_trajectories[i]\n",
    "    ax = axes[i + 3]\n",
    "    \n",
    "    # Extract positions\n",
    "    positions = np.array(traj.observations)[:, :2]\n",
    "    goal = traj.goals[0]\n",
    "    \n",
    "    # Plot trajectory\n",
    "    ax.plot(positions[:, 0], positions[:, 1], 'b-', alpha=0.7, linewidth=2)\n",
    "    ax.scatter(positions[0, 0], positions[0, 1], c='green', s=100, marker='o', label='Start', zorder=5)\n",
    "    ax.scatter(positions[-1, 0], positions[-1, 1], c='red', s=100, marker='s', label='End', zorder=5)\n",
    "    ax.scatter(goal[0], goal[1], c='orange', s=100, marker='*', label='Goal', zorder=5)\n",
    "    \n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_title(f'Random Trajectory {i+1} (Success: {traj.success})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print trajectory statistics\n",
    "print(\"Trajectory Statistics:\")\n",
    "print(f\"Expert success rate: {sum(traj.success for traj in expert_trajectories) / len(expert_trajectories):.2%}\")\n",
    "print(f\"Random success rate: {sum(traj.success for traj in random_trajectories) / len(random_trajectories):.2%}\")\n",
    "print(f\"Expert avg length: {np.mean([traj.length for traj in expert_trajectories]):.1f}\")\n",
    "print(f\"Random avg length: {np.mean([traj.length for traj in random_trajectories]):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b24674",
   "metadata": {},
   "source": [
    "## 6. Usage Example for Offline RL Algorithms\n",
    "\n",
    "Here's how to use the data loader with your offline RL algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for training an offline RL algorithm\n",
    "def train_offline_rl_algorithm(train_loader, val_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Example training loop for offline RL algorithm.\n",
    "    Replace this with your actual algorithm (e.g., CQL, IQL, etc.)\n",
    "    \"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move to device if using GPU\n",
    "            # batch = batch.to(device)\n",
    "            \n",
    "            # Your algorithm's training step here\n",
    "            # loss = your_algorithm.train_step(batch)\n",
    "            # train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            # Move to device if using GPU\n",
    "            # batch = batch.to(device)\n",
    "            \n",
    "            # Your algorithm's validation step here\n",
    "            # loss = your_algorithm.val_step(batch)\n",
    "            # val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "        \n",
    "        # Print progress (replace with actual loss calculation)\n",
    "        avg_train_loss = train_loss / max(train_batches, 1)\n",
    "        avg_val_loss = val_loss / max(val_batches, 1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Run example training\n",
    "train_offline_rl_algorithm(train_loader, val_loader, num_epochs=5)\n",
    "\n",
    "print(\"\\nTraining complete! You can now implement your actual offline RL algorithm.\")\n",
    "print(\"The data loader provides:\")\n",
    "print(\"- Batched transitions with goal conditioning\")\n",
    "print(\"- Support for different goal sampling strategies\")\n",
    "print(\"- Easy integration with PyTorch training loops\")\n",
    "print(\"- Composable design for different algorithms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feff6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
